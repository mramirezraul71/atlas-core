# ATLAS LLM Hybrid Configuration
# Copy this file to .env and configure with your actual values

# LLM Mode: local, cloud, or hybrid
ATLAS_LLM_MODE=hybrid

# Default provider to try first: ollama or openai
ATLAS_LLM_DEFAULT=ollama

# Strict offline mode (1=only local, 0=allow cloud fallback)
ATLAS_LLM_STRICT_OFFLINE=0

# Ollama Configuration (Local LLM)
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=deepseek-r1:latest

# OpenAI Configuration (Cloud LLM)
OPENAI_API_KEY=sk-REPLACE_WITH_YOUR_ACTUAL_KEY
OPENAI_MODEL=gpt-4o-mini

# Supervisor Style
ATLAS_SUPERVISOR_STYLE=operational

# Audit Directory
ATLAS_AUDIT_DIR=C:\ATLAS\logs

# --- Open Interpreter (Workspace Execution Engine) ---
# Default model for interpreter (auto-detected from available providers if empty)
WORKSPACE_INTERPRETER_MODEL=
# Fast model for simple/quick tasks
WORKSPACE_INTERPRETER_MODEL_FAST=
# Auto-run code without confirmation (true/false)
INTERPRETER_AUTO_RUN=false
# Max concurrent interpreter sessions
INTERPRETER_MAX_SESSIONS=5
# Session time-to-live in seconds (inactive sessions are evicted)
INTERPRETER_SESSION_TTL_SEC=900

# --- Cloud Provider API Keys ---
# AWS Bedrock (uses AWS_ACCESS_KEY_ID + AWS_SECRET_ACCESS_KEY or AWS_PROFILE)
# ANTHROPIC_API_KEY=sk-ant-REPLACE
# GEMINI_API_KEY=REPLACE
# XAI_API_KEY=xai-REPLACE
# DEEPSEEK_API_KEY=sk-REPLACE
# GROQ_API_KEY=gsk_REPLACE
